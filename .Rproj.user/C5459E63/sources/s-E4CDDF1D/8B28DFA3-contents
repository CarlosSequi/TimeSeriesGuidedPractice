---
title: "practicaGuiada"
output: pdf_document
---


Cargamos la biblioteca para series temporales.
```{r}
library(tseries)
```

Leemos los datos de la serie
```{r}
serie = scan("pasajeros_1949_1959.dat")
```

Dividimos los datos en entrenamiento y test. Dejamos para ello el último año para la comprobación de la validez del modelo, y el resto lo usamos como train.
```{r}
NTest = 12 # cantidad de datos a usar como test
NPred = 12 # cantidad de predicciones que queremos realizar 
serie.ts = ts(serie, frequency = 12) # creamos el objeto serie temporal suponiendo una estacionalidad de 12 meses
plot(decompose(serie.ts))
```

Observamos aquí: \\
-observed: los valores de la serie\\
-trend: la tendencia calculada mediante filtros\\
-seasonal: estacionalidad en la que cada 12 instantes de tiempo se repite la serie\\
-random: lo que queda de la serie una vez eliminadas tendencia y estacionalidad\\

Como observamos en random, la varianza es alta, lo que puede dar problemas en un futuro para la estacionariedad,
ya que como sabemos, una serie con estacionariedad no varía en media ni en varianza. Para ello, a la serie inicial
le calculamos el logaritmo de la serie (calculamos el logaritmo tanto a los datos como a la serie temporal):
```{r}
serie.ts.log = log(serie.ts)
serie.log = log(serie)
plot(decompose(serie.ts.log))
```

Como observamos ahora, la varianza consta de menor variación a lo largo del tiempo, lo que en un futuro provocará que no tengamos problemas a la hora de calcular la estacionariedad.

Aplicando decompose sobre los datos podemos observar como los valores de cada mes en el atributo seasonal son exactamente los mismos, lo que nos hará falta para calcular la componente estacional y restarselo a la serie.
```{r}
decompose(serie.ts.log)
```


Procedemos a la división de los datos en train y test
```{r}
serieTr = serie.log[1:(length(serie.log)-NTest)] # tomamos todos los valores de la serie
                                                 # como train excepto los 12 últimos
tiempoTr = 1:length(serieTr) # instantes de tiempo de esta serie train

# mismo proceso para la serie de test...
serieTs = serie.log[(length(serie.log)-NTest+1):length(serie)]
tiempoTs = (tiempoTr[length(tiempoTr)]+1):(tiempoTr[length(tiempoTr)]+NTest)
```


Representamos la serie de entrenamiento y la linea de la serie de test en rojo con los parametros necesarios para que salga de forma correcta dentro de los límites de la gráfica y en el lugar adecuado.
```{r}
plot.ts(serieTr, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs, col="red")
```


A continuación prodecemos a modelar la tendencia. 
Parece ser que la tendencia es lineal y creciente, por lo que tendrá la forma de:\\
serie = parametroA*tiempo + parametroB.\\
Con la funcion lm calculamos esos dos parametros para modelar dicha tendencia.
```{r}
parametros.H1 = lm(serieTr ~tiempoTr)
parametros.H1
```

Intercept es el termino independiente (parametroB) y el otro es el que multiplica al tiempo para poder calcular la serie (parametroA). Para modelar la tendencia usamos la fórmula descrita antes:
```{r}
tendEstimadaTr = parametros.H1$coefficients[1] + tiempoTr*parametros.H1$coefficients[2] # tendencia en entrenamiento
tendEstimadaTs = parametros.H1$coefficients[1] + tiempoTs*parametros.H1$coefficients[2] # tendencia en test
```

Comprobamos de forma visual si la tendencia se ajusta al modelo que tenemos de la serie temporal.
```{r}
plot.ts(serieTr, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs, col="red")
lines(tiempoTr, tendEstimadaTr, col = "blue")
lines(tiempoTs, tendEstimadaTs, col = "green")
```


Validamos a continuación de forma estadística que el modelo lineal creado sea correcto. Para ellos hemos de comprobar que los errores a lo largo del rango de la serie tanto en entrenamiento como en test son normales, es decir, que se distribuyen mediante distribución normal a lo largo de todo el tiempo.\\

Aplicamos el test de normalidad de Jarque Bera para comprobar la normalidad en los errores tanto en los residuos del entrenamiento como en los del test, calculados como la tendencia estimada en test menos la serie temporal.


```{r}
JB.tr = jarque.bera.test(parametros.H1$residuals)
JB.ts = jarque.bera.test(tendEstimadaTs-serieTs)
JB.tr
JB.ts
```

Asumiendo confianza del 95% podemos asumir que no hay diferencia significativa de los datos de error con respecto a los de una distribución normal ni en train ni en test (ya que ambos p-value son mayores a 0.05).\\

Comparamos las medias de error para comprobar si el error producido en la parte de train es equivalente al producido en la parte de test. Aplicamos el Test de Student para comparar dos distribuciones de datos diferentes (mediante sus medias).
```{r}
TT = t.test(c(parametros.H1$residuals, tendEstimadaTs-serieTs))
TT
```

El p-value nos indica que esta distribución de errores es de media cero (casi: 0.007), y que no hay una desviación significativa con respecto a esta media, por tanto ambos errores de test y entrenamiento tienen la misma media y el modelo lineal es válido, por lo que la tendencia calculada es correcta. Procedemos por tanto a eliminar la tendencia de las series iniciales de entrenamiento y test.

```{r}
serieTr.SinTend = serieTr - tendEstimadaTr
serieTs.SinTend = serieTs - tendEstimadaTs

# comprobamos como queda:
plot.ts(serieTr.SinTend, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs.SinTend, col="red")
```


Una vez eliminda la tendencia quitamos la estacionalidad. Usamos para ello la funcion decompose
```{r}
k = 12 # aquí guardamos los datos estacionales
estacionalidad = decompose(serie.ts.log)$seasonal[1:k] # tal como dijimos antes usamos el atributo seasonal
                                                       # para calcular los k=12 valores de estacionalidad
                                                       # que se van repitiendo en la estacionalidad
```

Ahora mismo tenemos una serie sin tendencia con 120 valores (train) y una estacionalidad con 12 valores. A esta serie sin tendencia hemos de restarle la estacionalidad de 12 en 12 valores (serie[1:12] - estacionalidad, serie[13:25] - estacionalidad ...).
```{r}
# repetimos la estacionalidad para ello, tantas veces como valores haya en la serie
aux = rep(estacionalidad, length(serieTr.SinTend)/length(estacionalidad))
# quitamos la estacionalidad a train y test
serieTr.SinTend.SinEst = serieTr.SinTend - aux
serieTs.SinTend.SinEst = serieTs.SinTend - estacionalidad # no usamos aux porque estacionalidad 
                                                          # tiene la misma cantidad de valores que
                                                          # serieTs.SinTend
# comprobamos como queda:
plot.ts(serieTr.SinTend.SinEst, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs.SinTend.SinEst, col="red")
```

Comprobamos si la serie es estacionaria o no con el test ACF
```{r}
acf(serieTr.SinTend.SinEst)
```

No llega a ser un resultado visualmente aclaratorio para determinar si posee o no estacionariedad debido a que posee una bajada muy suave entre instantes de tiempo, por tanto procedemos a aplicar un test de Dickey-Fuller aumentado para asegurarnos.

```{r}
ADFTr = adf.test(serieTr.SinTend.SinEst)
ADFTr
```

Con un nivel de confianza del 95% no podemos asegurar que la serie sea estacionaria. Lo normal es no sea estacionario en media, es decir, que la media a lo largo del tiempo varie. Debemos hacer la serie estacionaria, por lo que vamos a diferenciarla.
```{r}
serieTr.SinTend.SinEst.Diff = diff(serieTr.SinTend.SinEst) # diferenciación en una unidad de la serie anterior
serieTs.SinTend.SinEst.Diff = diff(serieTs.SinTend.SinEst)

# le pasamos el test de nuevo para comprobar si es estacionaria
ADFTr2 = adf.test(serieTr.SinTend.SinEst.Diff)
ADFTr2
```

Al ser p-value < 0.05 podemos asumir que ya sí es estacionaria.
Lo comprobamos también de forma visual:
```{r}
acf(serieTr.SinTend.SinEst.Diff)
```

Observamos la existencia de estacionaridad con la gran bajada entre los instantes 0 y 1.

Observamos el acf parcial para ver como influyen de forma individual cada uno de los instantes de tiempo sobre
el instante actual.
```{r}
pacf(serieTr.SinTend.SinEst.Diff)
```

Podemos pensar que estas gráficas de acf y pacf son típicos de un modelo autoregresivo de grado 4 (el último instante donde la línea pasa el umbral en pacf). Por ello deberíamos poder modelar el sistema mediante un modelo autoregresivo de orden 4. Podemos usar para ello un modelo ARIMA con una diferenciación. Entrenamos dicho modelo a continuación.
```{r}
# calculamos el modelo
# la diferenciación la introducimos dentro del modelo, por tanto usamos serieTr.SinTend.SinEst
modelo = arima(serieTr.SinTend.SinEst, order = c(4,1,0)) # 0 indica que no es de medias moviles
# vemos los residuos del modelo, que al ser un modelo autoregresivo lo calculamos como residuos+serie
valoresReconstruidos = serieTr.SinTend.SinEst + modelo$residuals # valores reconstruidos de la serie
                                                                 # con el modelo que tenemos

# calculamos las predicciones
predicciones = predict(modelo, n.ahead = NPred)
valoresPredichos = predicciones$pred
valoresPredichos

# calculamos el error cuadrático acumulado del ajuste en ajuste y test
errorTr = sum((modelo$residuals)^2)
errorTs = sum((valoresPredichos - serieTs.SinTend.SinEst) ^2)
errorTr
errorTs
```

























