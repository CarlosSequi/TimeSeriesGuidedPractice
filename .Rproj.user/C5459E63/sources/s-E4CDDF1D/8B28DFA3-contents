---
title: "Práctica Guiada"
output: pdf_document
author: "Carlos Manuel Sequí Sánchez"
---


Cargamos la biblioteca para series temporales.
```{r}
library(tseries)
```

Leemos los datos de la serie
```{r}
serie = scan("pasajeros_1949_1959.dat")
```

Dividimos los datos en entrenamiento y test. Dejamos para ello el último año para la comprobación de la validez del modelo, y el resto lo usamos como train.
```{r}
NTest = 12 # cantidad de datos a usar como test
NPred = 12 # cantidad de predicciones que queremos realizar 
serie.ts = ts(serie, frequency = 12) # creamos el objeto serie temporal
                                     # suponiendo una estacionalidad de 12 meses
plot(decompose(serie.ts))
```

Observamos aquí:

-observed: los valores de la serie  
-trend: la tendencia calculada mediante filtros  
-seasonal: estacionalidad en la que cada 12 instantes de tiempo se repite la serie  
-random: lo que queda de la serie una vez eliminadas tendencia y estacionalidad  

Como observamos en random, la varianza es alta, lo que puede dar problemas en un futuro para la estacionariedad,
ya que como sabemos, una serie con estacionariedad no varía en media ni en varianza. Para ello, a la serie inicial
le calculamos el logaritmo de la serie (calculamos el logaritmo tanto a los datos como a la serie temporal):
```{r}
serie.ts.log = log(serie.ts)
serie.log = log(serie)
plot(decompose(serie.ts.log))
```

Como observamos ahora, la varianza consta de menor variación a lo largo del tiempo, lo que en un futuro provocará que no tengamos problemas a la hora de calcular la estacionariedad.

Aplicando decompose sobre los datos podemos observar como los valores de cada mes en el atributo seasonal son exactamente los mismos, lo que nos hará falta para calcular la componente estacional y restarselo a la serie.
```{r}
decompose(serie.ts.log)
```


Procedemos a la división de los datos en train y test
```{r}
serieTr = serie.log[1:(length(serie.log)-NTest)] # tomamos todos los valores de la serie
                                                 # como train excepto los 12 últimos
tiempoTr = 1:length(serieTr) # instantes de tiempo de esta serie train

# mismo proceso para la serie de test...
serieTs = serie.log[(length(serie.log)-NTest+1):length(serie)]
tiempoTs = (tiempoTr[length(tiempoTr)]+1):(tiempoTr[length(tiempoTr)]+NTest)
```


Representamos la serie de entrenamiento y la linea de la serie de test en rojo con los parametros necesarios para que salga de forma correcta dentro de los límites de la gráfica y en el lugar adecuado.
```{r}
plot.ts(serieTr, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs, col="red")
```


A continuación prodecemos a modelar la tendencia. 
Parece ser que la tendencia es lineal y creciente, por lo que tendrá la forma de:  
serie = parametroA*tiempo + parametroB.  
Con la funcion lm calculamos esos dos parametros para modelar dicha tendencia.
```{r}
parametros.H1 = lm(serieTr ~tiempoTr)
parametros.H1
```

Intercept es el termino independiente (parametroB) y el otro es el que multiplica al tiempo para poder calcular la serie (parametroA). Para modelar la tendencia usamos la fórmula descrita antes:
```{r}
# tendencia en entrenamiento
tendEstimadaTr = parametros.H1$coefficients[1] + tiempoTr*parametros.H1$coefficients[2]
# tendencia en test
tendEstimadaTs = parametros.H1$coefficients[1] + tiempoTs*parametros.H1$coefficients[2] 
```

Comprobamos de forma visual si la tendencia se ajusta al modelo que tenemos de la serie temporal.
```{r}
plot.ts(serieTr, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs, col="red")
lines(tiempoTr, tendEstimadaTr, col = "blue")
lines(tiempoTs, tendEstimadaTs, col = "green")
```


Validamos a continuación de forma estadística que el modelo lineal creado sea correcto. Para ellos hemos de comprobar que los errores a lo largo del rango de la serie tanto en entrenamiento como en test son normales, es decir, que se distribuyen mediante distribución normal a lo largo de todo el tiempo.

Aplicamos el test de normalidad de Jarque Bera para comprobar la normalidad en los errores tanto en los residuos del entrenamiento como en los del test, calculados como la tendencia estimada en test menos la serie temporal.


```{r}
JB.tr = jarque.bera.test(parametros.H1$residuals)
JB.ts = jarque.bera.test(tendEstimadaTs-serieTs)
JB.tr
JB.ts
```

Asumiendo confianza del 95% podemos asumir que no hay diferencia significativa de los datos de error con respecto a los de una distribución normal ni en train ni en test (ya que ambos p-value son mayores a 0.05).

Comparamos las medias de error para comprobar si el error producido en la parte de train es equivalente al producido en la parte de test. Aplicamos el Test de Student para comparar dos distribuciones de datos diferentes (mediante sus medias).
```{r}
TT = t.test(c(parametros.H1$residuals, tendEstimadaTs-serieTs))
TT
```

El p-value nos indica que esta distribución de errores es de media cero (casi: 0.007), y que no hay una desviación significativa con respecto a esta media, por tanto ambos errores de test y entrenamiento tienen la misma media y el modelo lineal es válido, por lo que la tendencia calculada es correcta. Procedemos por tanto a eliminar la tendencia de las series iniciales de entrenamiento y test.

```{r}
serieTr.SinTend = serieTr - tendEstimadaTr
serieTs.SinTend = serieTs - tendEstimadaTs

# comprobamos como queda:
plot.ts(serieTr.SinTend, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs.SinTend, col="red")
```


Una vez eliminda la tendencia quitamos la estacionalidad. Usamos para ello la funcion decompose
```{r}
k = 12 # aquí guardamos los datos estacionales
estacionalidad = decompose(serie.ts.log)$seasonal[1:k] # tal como dijimos antes usamos 
                                                       # el atributo seasonal
                                                       # para calcular los k=12 valores
                                                       # de estacionalidad
                                                       # que se van repitiendo en la estacionalidad
```

Ahora mismo tenemos una serie sin tendencia con 120 valores (train) y una estacionalidad con 12 valores. A esta serie sin tendencia hemos de restarle la estacionalidad de 12 en 12 valores (serie[1:12] - estacionalidad, serie[13:25] - estacionalidad ...).
```{r}
# repetimos la estacionalidad para ello, tantas veces como valores haya en la serie
aux = rep(estacionalidad, length(serieTr.SinTend)/length(estacionalidad))
# quitamos la estacionalidad a train y test
serieTr.SinTend.SinEst = serieTr.SinTend - aux
serieTs.SinTend.SinEst = serieTs.SinTend - estacionalidad # no usamos aux porque estacionalidad 
                                                          # tiene la misma cantidad de valores que
                                                          # serieTs.SinTend
# comprobamos como queda:
plot.ts(serieTr.SinTend.SinEst, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(tiempoTs, serieTs.SinTend.SinEst, col="red")
```

Comprobamos si la serie es estacionaria o no con el test ACF
```{r}
acf(serieTr.SinTend.SinEst)
```

No llega a ser un resultado visualmente aclaratorio para determinar si posee o no estacionariedad debido a que posee una bajada muy suave entre instantes de tiempo, por tanto procedemos a aplicar un test de Dickey-Fuller aumentado para asegurarnos.

```{r}
ADFTr = adf.test(serieTr.SinTend.SinEst)
ADFTr
```

Con un nivel de confianza del 95% no podemos asegurar que la serie sea estacionaria. Lo normal es no sea estacionario en media, es decir, que la media a lo largo del tiempo varie. Debemos hacer la serie estacionaria, por lo que vamos a diferenciarla.
```{r}
serieTr.SinTend.SinEst.Diff = diff(serieTr.SinTend.SinEst) # diferenciación en una unidad 
                                                           # de la serie anterior
serieTs.SinTend.SinEst.Diff = diff(serieTs.SinTend.SinEst)

# le pasamos el test de nuevo para comprobar si es estacionaria
ADFTr2 = adf.test(serieTr.SinTend.SinEst.Diff)
ADFTr2
```

Al ser p-value < 0.05 podemos asumir que ya sí es estacionaria.
Lo comprobamos también de forma visual:
```{r}
acf(serieTr.SinTend.SinEst.Diff)
```

Observamos la existencia de estacionaridad con la gran bajada entre los instantes 0 y 1.

Observamos el acf parcial para ver como influyen de forma individual cada uno de los instantes de tiempo sobre
el instante actual.
```{r}
pacf(serieTr.SinTend.SinEst.Diff)
```

Podemos pensar que estas gráficas de acf y pacf son típicos de un modelo autoregresivo de grado 4 (el último instante donde la línea pasa el umbral en pacf). Por ello deberíamos poder modelar el sistema mediante un modelo autoregresivo de orden 4. Podemos usar para ello un modelo ARIMA con una diferenciación. Entrenamos dicho modelo a continuación.
```{r}
# calculamos el modelo
# la diferenciación la introducimos dentro del modelo, por tanto usamos serieTr.SinTend.SinEst
modelo = arima(serieTr.SinTend.SinEst, order = c(4,1,0)) # 0 indica que no es de medias moviles
# vemos los residuos del modelo, que al ser un modelo autoregresivo lo calculamos como residuos+serie
valoresReconstruidos = serieTr.SinTend.SinEst + modelo$residuals # valores reconstruidos de la serie
                                                                 # con el modelo que tenemos

# calculamos las predicciones
predicciones = predict(modelo, n.ahead = NPred)
valoresPredichos = predicciones$pred
valoresPredichos

# calculamos el error cuadrático acumulado del ajuste en ajuste y test
errorTr = sum((modelo$residuals)^2)
errorTs = sum((valoresPredichos - serieTs.SinTend.SinEst) ^2)
errorTr
errorTs
```

Comprobamos de forma visual el modelo incluyendo la serie sin tendencia ni estacionalidad
```{r}
plot.ts(serieTr.SinTend.SinEst, xlim=c(1, tiempoTs[length(tiempoTs)]))
lines(valoresReconstruidos, col="blue")
lines(tiempoTs,serieTs.SinTend.SinEst, col="red")
lines(tiempoTs,valoresPredichos, col="green")
```

La reconstrucción para el train es razonablemente fiel, sin embargo para el test no, posiblemente debido a que, al observar acf y pacf, podemos ver que los instantes distintos al actual están muy cercanos a los límites, por lo que quizás no sean lo suficientemente importantes para calcular el valor en el instante actual. Podemos decir que nuestra serie es prácticamente ruido blanco, por ello nos aparece la predicción verde observada en la gráfica. Finalmente comprobamos la validez del modelo (sus errores) mediante tests estadisticos. Para ello aplicamos varios:


Test de Box-Pierce: comprobamos que los residuos que nos quedan son aleatorios
```{r}
BTtest = Box.test(modelo$residuals)
BTtest
```

Con una confianza del 95% no podemos pensar que los residuos no sean valores aleatorios, por tanto pasa el test determinando que sí, siguen una distribución aleatoria. Si no fuesen aleatorios significaría que nuestro modelo no se ajustaría a todas las características de la señal, por lo que sería un modelo incorrecto. En este caso es válido con respecto al valor de los residuos.


Jarque Bera: vemos si los residuos, aunque aleatorios, son normales (media cero y desviación típica la que sea).
```{r}
JBt = jarque.bera.test(modelo$residuals)
JBt
```


Saphiro-Wilk: mismo cometido que Jarque Bera
```{r}
swt = shapiro.test(modelo$residuals)
swt
```

Asumimos en ambos la normalidad de los residuos con el resultado de los p-value obtenidos, ya que la hipótesis nula propone que los datos no siguen una distribución normal.

A continuación mostramos un histograma y función de densidad para obtener resultados gráficos de confirmación.
```{r}
hist(modelo$residuals, col="blue", prob=TRUE, ylim=c(0,20), xlim=c(-0.2,0.2))
lines(density(modelo$residuals))
```


Con estas comprobaciones el modelo queda validado, ya que los errores que nos dan son aleatorios que provienen de una distribución normal. A continuación hemos de deshacer los cambios para comprobar la predicción con el conjunto de datos de test. Procedemos a ello:
```{r}
# Incluimos la estacionalidad
valoresReconstruidos.Est = valoresReconstruidos + aux
valoresPredichos.Est = valoresPredichos + estacionalidad

# Incluimos la tendencia
valoresReconstruidos.Est.Tend = valoresReconstruidos.Est + tendEstimadaTr
valoresPredichos.Est.Tend = valoresPredichos.Est + tendEstimadaTs

# Transformación de los logaritmos
valoresReconstruidos.Est.Tend.exp = exp(valoresReconstruidos.Est.Tend)
valoresPredichos.Est.Tend.exp = exp(valoresPredichos.Est.Tend)

plot.ts(serie)
lines(tiempoTr, valoresReconstruidos.Est.Tend.exp, col = "blue")
lines(tiempoTs, valoresPredichos.Est.Tend.exp, col = "green")
```

















